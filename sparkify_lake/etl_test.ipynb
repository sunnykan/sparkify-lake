{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280c3f30-f19a-41d0-99c4-3cf5e153aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd7c489-90ea-4269-af93-524b984120b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38434cf6-d296-47da-a7ae-12ef41a72697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.221:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a3cc580>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b53f24b3-5f50-4bdb-83f8-12c6b696535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data) -> None:\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data \n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "    print(df.printSchema())\n",
    "    \n",
    "    df.createOrReplaceTempView(\"song_data_table\")\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    query = \"\"\"select song_id, artist_id, year, duration\n",
    "    from song_data_table;\n",
    "    \"\"\"\n",
    "    \n",
    "    songs_table = spark.sql(query)\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy(\"year\", \"artist_id\").mode(\"overwrite\").parquet(output_data + \"songs/songs_table.parquet\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    query = \"\"\"select artist_id, artist_name, artist_latitude, artist_longitude\n",
    "    from song_data_table;\n",
    "    \"\"\"\n",
    "\n",
    "    artists_table = spark.sql(query)\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode(\"overwrite\").parquet(output_data + \"artists/artists_table.parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b76611a6-bc6e-41fc-a211-f0213c646785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = \"../data/log_data/*.json\"\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "    print(df.printSchema())\n",
    "    \n",
    "    df.createOrReplaceTempView(\"log_data_table\")\n",
    "    \n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    query = \"\"\"with songplay_data as\n",
    "    (\n",
    "        select e.userId as user_id, \n",
    "            s.song_id as song_id, \n",
    "            s.artist_id as artist_id, \n",
    "            e.sessionid as session_id, \n",
    "            e.ts as start_time, \n",
    "            e.level as level, \n",
    "            e.location as location, \n",
    "            e.userAgent as user_agent,\n",
    "            year(TIMESTAMP 'epoch' + e.ts/1000 * INTERVAL '1 second') as year,\n",
    "            month(TIMESTAMP 'epoch' + e.ts/1000 * INTERVAL '1 second') as month\n",
    "        from log_data_table e\n",
    "        join song_data_table s\n",
    "        on s.title = e.song \n",
    "            and s.artist_name = e.artist\n",
    "                and round(s.duration) = round(e.length)\n",
    "        where page = 'NextSong'\n",
    "    ) \n",
    "    select row_number() over (order by \"monotonically_increasing_id\") as songplay_id, * \n",
    "    \n",
    "    from songplay_data;\n",
    "    \"\"\"\n",
    "    \n",
    "    songplays_table = spark.sql(query)\n",
    "    songplays_table.createOrReplaceTempView(\"songplays_data_table\")\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_data + \"songplays/songplays_table.parquet\")\n",
    "    \n",
    "    # extract columns for users table\n",
    "    \n",
    "    query = \"\"\"with user_data (\n",
    "        select userId, firstName, lastName, gender, level, row_number() over (partition by userId order by ts desc) as obs_num\n",
    "        from log_data_table\n",
    "        where page = 'NextSong'\n",
    "    )\n",
    "    select userId as user_id,\n",
    "        firstName as first_name,\n",
    "        lastName as last_name,\n",
    "        gender,\n",
    "        level,\n",
    "        obs_num\n",
    "    from user_data\n",
    "    where obs_num = 1;\n",
    "    \"\"\"\n",
    "    users_table = spark.sql(query)\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\").parquet(output_data + \"users/users_table.parquet\")\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    query = \"\"\"select distinct start_time,\n",
    "                    hour(TIMESTAMP 'epoch' + start_time/1000 * INTERVAL '1 second') as hour,\n",
    "                    dayofyear(TIMESTAMP 'epoch' + start_time/1000 * INTERVAL '1 second') as day,\n",
    "                    weekofyear(TIMESTAMP 'epoch' + start_time/1000 * INTERVAL '1 second') as week,\n",
    "                    month(TIMESTAMP 'epoch' + start_time/1000 * INTERVAL '1 second') as month,\n",
    "                    year(TIMESTAMP 'epoch' + start_time/1000 * INTERVAL '1 second') as year,\n",
    "                    dayofweek(TIMESTAMP 'epoch' + start_time/1000 * INTERVAL '1 second') as weekday        \n",
    "                from songplays_data_table;\n",
    "    \"\"\"\n",
    "    time_table = spark.sql(query)\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").parquet(output_data + \"time/time_table.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a6424dca-b3a3-4342-b3ab-fa7a637c050a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb146643-bfa4-443e-886e-8f2e96cf0947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
